#!/usr/bin/env python3
# -*- coding: utf-8 -*-
"""
arXiv Robotics è®ºæ–‡æ¯æ—¥ç›‘æ§
ç­›é€‰æ–¹å‘ï¼šLifelong/Long-term SLAM, Navigation, é“°æ¥å¼ç‰©ä½“æ“ä½œ
ä½¿ç”¨ DeepSeek è¿›è¡Œä¸­æ–‡æ‘˜è¦ç¿»è¯‘

ç”¨æˆ·è¦æ±‚ï¼š
1. æŠ¥å‘Šå‘½åï¼šdaily_report_{YYYY-MM-DD}.mdï¼ˆä¸è¦†ç›–å†å²æ•°æ®ï¼‰
2. å®Œæ•´ä½œè€…åˆ—è¡¨ + é™„å±å•ä½
3. åˆ†ææ˜¯å¦æä¾›å¼€æºæ¨¡å‹/ä»£ç 
4. å›å¤å½“å‰æœç´¢å…³é”®è¯

é…ç½®è¯´æ˜ï¼š
- æ‰€æœ‰é…ç½®é¡¹å­˜å‚¨åœ¨ config.json ä¸­
- ä¿®æ”¹é…ç½®è¯·ç¼–è¾‘ config.json æ–‡ä»¶
"""

import json
import urllib.request
import urllib.parse
from datetime import datetime, timedelta
from pathlib import Path
import http.client
import ssl
import subprocess
import shutil
import os
import re

# ============================================================================
# é…ç½®åŠ è½½
# ============================================================================

CONFIG_PATH = Path(__file__).parent / 'config.json'

def load_config():
    """ä» config.json åŠ è½½é…ç½®"""
    try:
        with open(CONFIG_PATH, 'r', encoding='utf-8') as f:
            config = json.load(f)
        return config
    except Exception as e:
        print(f"âš ï¸ è¯»å– config.json å¤±è´¥ï¼š{e}")
        return {}

def load_deepseek_config(config):
    """ä» config.json åŠ è½½ DeepSeek é…ç½®"""
    nanobot_config_path = Path('/home/jjiao/.nanobot/config.json')
    try:
        with open(nanobot_config_path, 'r', encoding='utf-8') as f:
            nanobot_config = json.load(f)
        providers = nanobot_config.get('providers', {})
        deepseek = providers.get('deepseek', {})
        return {
            'api_key': deepseek.get('apiKey', ''),
            'api_base': deepseek.get('apiBase', 'https://api.deepseek.com'),
            'model': deepseek.get('model', 'deepseek-chat')
        }
    except Exception as e:
        print(f"âš ï¸ è¯»å– nanobot config.json å¤±è´¥ï¼š{e}")
        return {
            'api_key': '',
            'api_base': 'https://api.deepseek.com',
            'model': 'deepseek-chat'
        }

# åŠ è½½å…¨å±€é…ç½®
CONFIG = load_config()

# DeepSeek é…ç½®
deepseek_config = load_deepseek_config(CONFIG)
DEEPSEEK_API_KEY = deepseek_config['api_key']
DEEPSEEK_API_BASE = deepseek_config['api_base'] if deepseek_config['api_base'] else 'https://api.deepseek.com'
DEEPSEEK_MODEL = deepseek_config.get('model', 'deepseek-chat')

# arXiv åˆ†ç±»
ARXIV_CATEGORIES = CONFIG.get('arxiv_categories', ['cs.RO', 'cs.AI', 'cs.CV'])

# æœç´¢å…³é”®è¯
KEYWORDS = CONFIG.get('keywords', {})

# Highlight å…³é”®è¯ï¼ˆé«˜ä¼˜å…ˆçº§ï¼‰
HIGHLIGHT_KEYWORDS = CONFIG.get('highlight_keywords', [])

# Highlight ä½œè€…åˆ—è¡¨
HIGHLIGHT_AUTHORS = CONFIG.get('highlight_authors', [])

# è·¯å¾„é…ç½®
PATHS = CONFIG.get('paths', {})
WORKSPACE_DIR = Path(PATHS.get('workspace', '/home/jjiao/.nanobot/workspace'))
ARXIV_MONITOR_DIR = Path(PATHS.get('arxiv_monitor', '/home/jjiao/.nanobot/workspace/arxiv_monitor'))
PDF_STORAGE_DIR = Path(PATHS.get('pdf_storage', ARXIV_MONITOR_DIR / 'papers_pdf'))
GITIGNORE_PATH = Path(PATHS.get('gitignore', ARXIV_MONITOR_DIR / '.gitignore'))

# ç¡®ä¿ç›®å½•å­˜åœ¨
PDF_STORAGE_DIR.mkdir(parents=True, exist_ok=True)

# åˆ›å»º .gitignore æ–‡ä»¶
if not GITIGNORE_PATH.exists():
    with open(GITIGNORE_PATH, 'w') as f:
        f.write("# PDF æ–‡ä»¶ä¸æ¨é€åˆ° GitHub\npapers_pdf/\n*.pdf\n")

# è¿‡æ»¤é…ç½®
FILTERS = CONFIG.get('filters', {})
MAX_DAYS_OLD = FILTERS.get('max_days_old', 3)
MIN_PAPERS = FILTERS.get('min_papers', 13)
MAX_PAPERS = FILTERS.get('max_papers', 20)

# GitHub é…ç½®
GITHUB_CONFIG = CONFIG.get('github', {})
GITHUB_USERNAME = GITHUB_CONFIG.get('username', 'gogojjh')
GITHUB_REPO = GITHUB_CONFIG.get('repo_name', 'everyday_paper_news_clawbot')
GITHUB_BRANCH = GITHUB_CONFIG.get('branch', 'main')
GITHUB_TAG_PREFIX = GITHUB_CONFIG.get('tag_prefix', 'arxiv-daily-')

# ç¿»è¯‘é…ç½®
TRANSLATION_CONFIG = CONFIG.get('translation', {})
TRANSLATION_MAX_LENGTH = TRANSLATION_CONFIG.get('max_abstract_length', 2000)
TRANSLATION_TEMPERATURE = TRANSLATION_CONFIG.get('temperature', 0.3)
TRANSLATION_MAX_TOKENS = TRANSLATION_CONFIG.get('max_tokens', 1024)

# ============================================================================
# æ ¸å¿ƒåŠŸèƒ½å‡½æ•°
# ============================================================================

def translate_to_chinese(text, max_length=TRANSLATION_MAX_LENGTH):
    """ä½¿ç”¨ DeepSeek ç¿»è¯‘æ‘˜è¦ä¸ºä¸­æ–‡"""
    if not text or len(text.strip()) == 0:
        return "æ— æ‘˜è¦"
    
    if len(text) > max_length:
        text = text[:max_length] + "..."
    
    try:
        url = f"{DEEPSEEK_API_BASE}/v1/chat/completions"
        
        payload = {
            "model": DEEPSEEK_MODEL,
            "messages": [
                {
                    "role": "system",
                    "content": "ä½ æ˜¯ä¸€ä½ä¸“ä¸šçš„å­¦æœ¯è®ºæ–‡ç¿»è¯‘åŠ©æ‰‹ã€‚è¯·å°†è‹±æ–‡æ‘˜è¦ç¿»è¯‘æˆæµç•…ã€å‡†ç¡®çš„ä¸­æ–‡ï¼Œä¿æŒä¸“ä¸šæœ¯è¯­çš„å‡†ç¡®æ€§ã€‚åªè¾“å‡ºç¿»è¯‘ç»“æœï¼Œä¸è¦æ·»åŠ ä»»ä½•è§£é‡Šã€‚"
                },
                {
                    "role": "user",
                    "content": f"è¯·å°†ä»¥ä¸‹è®ºæ–‡æ‘˜è¦ç¿»è¯‘æˆä¸­æ–‡ï¼š\n\n{text}"
                }
            ],
            "temperature": TRANSLATION_TEMPERATURE,
            "max_tokens": TRANSLATION_MAX_TOKENS
        }
        
        headers = {
            "Content-Type": "application/json",
            "Authorization": f"Bearer {DEEPSEEK_API_KEY}"
        }
        
        data = json.dumps(payload).encode('utf-8')
        req = urllib.request.Request(url, data=data, headers=headers, method='POST')
        
        context = ssl.create_default_context()
        context.check_hostname = False
        context.verify_mode = ssl.CERT_NONE
        
        with urllib.request.urlopen(req, timeout=60, context=context) as response:
            result = json.loads(response.read().decode('utf-8'))
        
        translation = result.get('choices', [{}])[0].get('message', {}).get('content', '')
        return translation.strip() if translation else "ç¿»è¯‘å¤±è´¥"
        
    except Exception as e:
        print(f"  âš ï¸ ç¿»è¯‘å¤±è´¥ï¼š{e}")
        return f"[ç¿»è¯‘å¤±è´¥ï¼š{str(e)[:50]}...]"

def parse_arxiv_response(xml_data):
    """è§£æ arXiv API çš„ XML å“åº”ï¼ˆåŒ…å«ä½œè€…å•ä½ï¼‰"""
    import xml.etree.ElementTree as ET
    
    papers = []
    try:
        root = ET.fromstring(xml_data)
        ns = {'atom': 'http://www.w3.org/2005/Atom',
              'arxiv': 'http://arxiv.org/schemas/atom'}
        
        for entry in root.findall('atom:entry', ns):
            title_elem = entry.find('atom:title', ns)
            summary_elem = entry.find('atom:summary', ns)
            published_elem = entry.find('atom:published', ns)
            id_elem = entry.find('atom:id', ns)
            
            authors = []
            affiliations = []
            for author in entry.findall('atom:author', ns):
                name_elem = author.find('atom:name', ns)
                if name_elem is not None and name_elem.text:
                    authors.append(name_elem.text.strip())
                aff_elem = author.find('arxiv:affiliation', ns)
                if aff_elem is not None and aff_elem.text:
                    affiliations.append(aff_elem.text.strip())
            
            if title_elem is not None and summary_elem is not None:
                paper = {
                    'title': title_elem.text.strip() if title_elem.text else '',
                    'summary': summary_elem.text.strip() if summary_elem.text else '',
                    'published': published_elem.text if published_elem is not None else '',
                    'arxiv_id': id_elem.text if id_elem is not None else '',
                    'authors': authors,
                    'affiliations': affiliations if affiliations else ["é™„å±å•ä½æœªæä¾›"]
                }
                papers.append(paper)
    except Exception as e:
        print(f"  âš ï¸ XML è§£æå¤±è´¥ï¼š{e}")
    
    return papers

def search_arxiv(query, max_results=50):
    """æœç´¢ arXiv API"""
    base_url = "http://export.arxiv.org/api/query?"
    
    search_query = urllib.parse.quote(f"(cat:{' OR cat:'.join(ARXIV_CATEGORIES)}) AND ({query})")
    url = f"{base_url}search_query={search_query}&max_results={max_results}&sortBy=submittedDate&sortOrder=descending"
    
    try:
        with urllib.request.urlopen(url, timeout=30) as response:
            data = response.read().decode('utf-8')
        return parse_arxiv_response(data)
    except Exception as e:
        print(f"æœç´¢å¤±è´¥ï¼š{e}")
        return []

def detect_open_source(paper):
    """æ£€æµ‹è®ºæ–‡æ˜¯å¦æä¾›å¼€æºä»£ç /æ¨¡å‹"""
    title = paper.get('title', '').lower()
    summary = paper.get('summary', '').lower()
    text = title + ' ' + summary
    
    open_source_indicators = {
        'github': ['github.com', 'github repo', 'github repository', 'our github'],
        'gitlab': ['gitlab.com', 'gitlab repo'],
        'code': ['code available', 'source code', 'open source', 'opensource', 
                 'open-source', 'publicly available', 'code repository'],
        'model': ['pre-trained model', 'model weights', 'checkpoints available', 
                  'model available', 'download model'],
        'project_page': ['project page', 'project website', 'demo page'],
        'huggingface': ['huggingface', 'hugging face', 'hf.co']
    }
    
    found = []
    for category, keywords in open_source_indicators.items():
        for kw in keywords:
            if kw in text:
                found.append(category.upper())
                break
    
    urls = []
    url_pattern = r'https?://[^\s<>"{}|\\^`\[\]]+'
    matches = re.findall(url_pattern, paper.get('summary', ''))
    for url in matches:
        if any(domain in url.lower() for domain in ['github', 'gitlab', 'huggingface']):
            urls.append(url)
    
    return {
        'has_open_source': len(found) > 0,
        'categories': list(set(found)),
        'urls': urls[:3]
    }

def download_pdf(paper, storage_dir=PDF_STORAGE_DIR):
    """ä¸‹è½½é«˜äº®è®ºæ–‡çš„ PDF æ–‡ä»¶"""
    arxiv_id = paper.get('arxiv_id', '')
    title = paper.get('title', '')
    
    if not arxiv_id:
        print(f"  âš ï¸ ç¼ºå°‘ arxiv_idï¼Œè·³è¿‡ä¸‹è½½")
        return None
    
    arxiv_id_clean = arxiv_id.split('/')[-1]
    pdf_url = f"https://arxiv.org/pdf/{arxiv_id_clean}.pdf"
    
    safe_title = re.sub(r'[<>:"/\\|ï¼Ÿ*]', '', title)
    safe_title = safe_title.replace(' ', '_')
    if len(safe_title) > 100:
        safe_title = safe_title[:100]
    
    filename = f"{arxiv_id_clean}_{safe_title}.pdf"
    pdf_path = storage_dir / filename
    
    if pdf_path.exists():
        print(f"  âœ… PDF å·²å­˜åœ¨ï¼š{filename}")
        return str(pdf_path)
    
    try:
        print(f"  ğŸ“¥ ä¸‹è½½ï¼š{pdf_url}")
        
        headers = {
            'User-Agent': 'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36'
        }
        req = urllib.request.Request(pdf_url, headers=headers)
        
        with urllib.request.urlopen(req, timeout=60) as response:
            with open(pdf_path, 'wb') as out_file:
                out_file.write(response.read())
        
        print(f"  âœ… ä¸‹è½½æˆåŠŸï¼š{filename}")
        return str(pdf_path)
        
    except Exception as e:
        print(f"  âŒ ä¸‹è½½å¤±è´¥ï¼š{e}")
        return None

def filter_papers(papers, keywords):
    """ç­›é€‰ç›¸å…³è®ºæ–‡"""
    filtered = []
    
    for paper in papers:
        title_lower = paper['title'].lower()
        summary_lower = paper['summary'].lower()
        text = title_lower + ' ' + summary_lower
        
        match_category = None
        match_keywords = []
        
        for category, kw_list in keywords.items():
            for kw in kw_list:
                if kw.lower() in text:
                    if match_category is None:
                        match_category = category
                    match_keywords.append(kw)
        
        if match_category:
            paper['match_category'] = match_category
            paper['match_keywords'] = list(set(match_keywords))
            filtered.append(paper)
    
    return filtered

def save_results(papers, output_path):
    """ä¿å­˜ç»“æœåˆ° JSON æ–‡ä»¶"""
    output_path.parent.mkdir(parents=True, exist_ok=True)
    
    existing_data = []
    if output_path.exists():
        try:
            with open(output_path, 'r', encoding='utf-8') as f:
                existing_data = json.load(f)
        except:
            existing_data = []
    
    existing_ids = {p.get('arxiv_id', '') for p in existing_data}
    new_papers = [p for p in papers if p.get('arxiv_id', '') not in existing_ids]
    
    all_papers = new_papers + existing_data
    all_papers.sort(key=lambda x: x.get('published', ''), reverse=True)
    
    with open(output_path, 'w', encoding='utf-8') as f:
        json.dump(all_papers, f, ensure_ascii=False, indent=2)
    
    return len(new_papers)

def is_highlight_paper(paper):
    """æ£€æŸ¥è®ºæ–‡æ˜¯å¦å±äº Highlight ç±»åˆ«"""
    title = paper.get('title', '').lower()
    summary = paper.get('summary', '').lower()
    text = title + ' ' + summary
    
    for kw in HIGHLIGHT_KEYWORDS:
        if kw.lower() in text:
            return True
    
    authors = paper.get('authors', [])
    for author in authors:
        if author in HIGHLIGHT_AUTHORS:
            return True
    
    return False

def generate_markdown_report(papers, report_path, search_keywords):
    """ç”Ÿæˆ Markdown æŠ¥å‘Š"""
    report_path.parent.mkdir(parents=True, exist_ok=True)
    
    today = datetime.now().strftime('%Y-%m-%d')
    
    highlight_papers = [p for p in papers if is_highlight_paper(p)]
    poster_papers = [p for p in papers if not is_highlight_paper(p)]
    
    with open(report_path, 'w', encoding='utf-8') as f:
        f.write(f"# ğŸ“š arXiv Robotics è®ºæ–‡æ—¥æŠ¥\n\n")
        f.write(f"**ç”Ÿæˆæ—¶é—´**: {datetime.now().strftime('%Y-%m-%d %H:%M')}\n\n")
        f.write(f"**ä»Šæ—¥æ–°å¢**: {len(papers)} ç¯‡\n\n")
        f.write(f"**æœç´¢å…³é”®è¯**: `{' | '.join(search_keywords)}`\n\n")
        f.write("---\n\n")
        
        open_source_count = sum(1 for p in papers if p.get('open_source', {}).get('has_open_source', False))
        f.write(f"**ğŸ”“ å¼€æºä»£ç /æ¨¡å‹**: {open_source_count}/{len(papers)} ç¯‡æä¾›\n\n")
        f.write(f"**ğŸŒŸ Highlight**: {len(highlight_papers)} ç¯‡ | **ğŸ“Œ Poster**: {len(poster_papers)} ç¯‡\n\n")
        f.write("---\n\n")
        
        if highlight_papers:
            f.write(f"## ğŸŒŸ Highlight\n\n")
            f.write(f"*äººå½¢æœºå™¨äºº | è¶³å¼æœºå™¨äºº | è§†è§‰å¯¼èˆª | ç»ˆèº«å¯¼èˆª | è§†è§‰å»ºå›¾ | çŸ¥åä½œè€…*\n\n")
            f.write("---\n\n")
            
            for i, paper in enumerate(highlight_papers, 1):
                f.write(f"### {i}. {paper['title']}\n\n")
                
                all_authors = ', '.join(paper['authors'])
                highlighted_author_names = [a for a in paper['authors'] if a in HIGHLIGHT_AUTHORS]
                
                if highlighted_author_names:
                    f.write(f"- **ä½œè€…**: {all_authors} â­\n")
                    f.write(f"  - **é«˜äº®ä½œè€…**: {', '.join(highlighted_author_names)}\n")
                else:
                    f.write(f"- **ä½œè€…**: {all_authors}\n")
                
                affiliations = paper.get('affiliations', [])
                if affiliations and affiliations[0] != "é™„å±å•ä½æœªæä¾›" and not affiliations[0].startswith("è·å–å¤±è´¥"):
                    f.write(f"- **å•ä½**: {'; '.join(affiliations[:3])}{'...' if len(affiliations) > 3 else ''}\n")
                else:
                    f.write(f"- **å•ä½**: è¯¦è§ [arXiv é¡µé¢]({paper['arxiv_id']})\n")
                
                f.write(f"- **å‘è¡¨æ—¥æœŸ**: {paper['published'][:10]}\n")
                f.write(f"- **åŒ¹é…å…³é”®è¯**: {', '.join(paper.get('match_keywords', []))}\n")
                f.write(f"- **arXiv**: [{paper['arxiv_id'].split('/')[-1]}]({paper['arxiv_id']})\n")
                
                if paper.get('pdf_path'):
                    pdf_name = Path(paper['pdf_path']).name
                    f.write(f"- **ğŸ“¥ PDF**: å·²ä¸‹è½½è‡³æœ¬åœ° (`{pdf_name}`)\n")
                
                open_source = paper.get('open_source', {})
                if open_source.get('has_open_source', False):
                    f.write(f"- **ğŸ”“ å¼€æº**: {', '.join(open_source.get('categories', []))}\n")
                    if open_source.get('urls'):
                        f.write(f"  - é“¾æ¥ï¼š{', '.join(open_source['urls'])}\n")
                else:
                    f.write(f"- **ğŸ”’ å¼€æº**: æœªæåŠ\n")
                
                chinese_summary = paper.get('chinese_summary', '')
                if chinese_summary:
                    f.write(f"- **ä¸­æ–‡æ‘˜è¦**: {chinese_summary}\n\n")
                else:
                    f.write(f"- **æ‘˜è¦**: {paper['summary'][:500]}{'...' if len(paper['summary']) > 500 else ''}\n\n")
                
                f.write("---\n\n")
        
        if poster_papers:
            f.write(f"## ğŸ“Œ Poster\n\n")
            f.write(f"*å…¶ä»–ç›¸å…³ç ”ç©¶*\n\n")
            f.write("---\n\n")
            
            for i, paper in enumerate(poster_papers, 1):
                f.write(f"### {i}. {paper['title']}\n\n")
                
                all_authors = ', '.join(paper['authors'])
                highlighted_author_names = [a for a in paper['authors'] if a in HIGHLIGHT_AUTHORS]
                
                if highlighted_author_names:
                    f.write(f"- **ä½œè€…**: {all_authors} â­\n")
                    f.write(f"  - **é«˜äº®ä½œè€…**: {', '.join(highlighted_author_names)}\n")
                else:
                    f.write(f"- **ä½œè€…**: {all_authors}\n")
                
                affiliations = paper.get('affiliations', [])
                if affiliations and affiliations[0] != "é™„å±å•ä½æœªæä¾›" and not affiliations[0].startswith("è·å–å¤±è´¥"):
                    f.write(f"- **å•ä½**: {'; '.join(affiliations[:3])}{'...' if len(affiliations) > 3 else ''}\n")
                else:
                    f.write(f"- **å•ä½**: è¯¦è§ [arXiv é¡µé¢]({paper['arxiv_id']})\n")
                
                f.write(f"- **å‘è¡¨æ—¥æœŸ**: {paper['published'][:10]}\n")
                f.write(f"- **åŒ¹é…å…³é”®è¯**: {', '.join(paper.get('match_keywords', []))}\n")
                f.write(f"- **arXiv**: [{paper['arxiv_id'].split('/')[-1]}]({paper['arxiv_id']})\n")
                
                if paper.get('pdf_path'):
                    pdf_name = Path(paper['pdf_path']).name
                    f.write(f"- **ğŸ“¥ PDF**: å·²ä¸‹è½½è‡³æœ¬åœ° (`{pdf_name}`)\n")
                else:
                    f.write(f"- **ğŸ“¥ PDF**: æœªä¸‹è½½ï¼ˆä»…é«˜äº®è®ºæ–‡è‡ªåŠ¨ä¸‹è½½ï¼‰\n")
                
                open_source = paper.get('open_source', {})
                if open_source.get('has_open_source', False):
                    f.write(f"- **ğŸ”“ å¼€æº**: {', '.join(open_source.get('categories', []))}\n")
                    if open_source.get('urls'):
                        f.write(f"  - é“¾æ¥ï¼š{', '.join(open_source['urls'])}\n")
                else:
                    f.write(f"- **ğŸ”’ å¼€æº**: æœªæåŠ\n")
                
                chinese_summary = paper.get('chinese_summary', '')
                if chinese_summary:
                    f.write(f"- **ä¸­æ–‡æ‘˜è¦**: {chinese_summary}\n\n")
                else:
                    f.write(f"- **æ‘˜è¦**: {paper['summary'][:500]}{'...' if len(paper['summary']) > 500 else ''}\n\n")
                
                f.write("---\n\n")
    
    return report_path

def update_readme(repo_local_path: str, papers_count: int):
    """æ›´æ–° README.md"""
    import re
    
    today = datetime.now().strftime('%Y-%m-%d')
    report_filename = f'arxiv_daily_report_{today}.md'
    readme_path = Path(repo_local_path) / 'README.md'
    
    if readme_path.exists():
        with open(readme_path, 'r', encoding='utf-8') as f:
            content = f.read()
    else:
        content = f"""# ğŸ“š arXiv Robotics Daily Report

æ¯æ—¥è‡ªåŠ¨æ¨é€ arXiv æœºå™¨äººå­¦ç›¸å…³è®ºæ–‡æ‘˜è¦å’Œç¿»è¯‘ã€‚

---

## ğŸ“„ å†å²æŠ¥å‘Š

| æ—¥æœŸ | æŠ¥å‘Šé“¾æ¥ | è®ºæ–‡æ•°é‡ |
|------|----------|----------|

---

## ğŸ“‹ æœç´¢èŒƒå›´

| åˆ†ç±» | è¯´æ˜ |
|------|------|
| **cs.RO** | Robotics |
| **cs.AI** | Artificial Intelligence |
| **cs.CV** | Computer Vision |

---

*æœ€åæ›´æ–°ï¼š{today}*
"""
    
    if f'[{report_filename}]' in content:
        print(f"âš ï¸ ä»Šæ—¥æŠ¥å‘Šå·²åœ¨ README ä¸­ï¼Œè·³è¿‡æ›´æ–°")
        return
    
    new_row = f"| {today} | [{report_filename}](./{report_filename}) | {papers_count} ç¯‡ |\n"
    
    table_header = "| æ—¥æœŸ | æŠ¥å‘Šé“¾æ¥ | è®ºæ–‡æ•°é‡ |\n|------|----------|----------|"
    if table_header in content:
        if table_header + "\n" in content:
            content = content.replace(table_header + "\n", table_header + "\n" + new_row, 1)
        else:
            content = content.replace(table_header, table_header + "\n" + new_row, 1)
    else:
        section_header = "## ğŸ“„ å†å²æŠ¥å‘Š"
        if section_header in content:
            idx = content.find(section_header) + len(section_header)
            content = content[:idx] + "\n\n| æ—¥æœŸ | æŠ¥å‘Šé“¾æ¥ | è®ºæ–‡æ•°é‡ |\n|------|----------|----------|\n" + new_row + content[idx:]
    
    content = re.sub(r'\*æœ€åæ›´æ–°ï¼š[^*]+\*', f'*æœ€åæ›´æ–°ï¼š{today}*', content)
    
    with open(readme_path, 'w', encoding='utf-8') as f:
        f.write(content)
    
    print(f"ğŸ“ README.md å·²æ›´æ–°")


def push_to_github(report_path: str, papers_count: int):
    """æ¨é€æ—¥æŠ¥åˆ° GitHub å¹¶åˆ›å»ºæ—¥æœŸ Tag"""
    
    config_path = Path('/home/jjiao/.nanobot/config.json')
    try:
        with open(config_path, 'r', encoding='utf-8') as f:
            config = json.load(f)
        github_config = config.get('integrations', {}).get('github', {})
        username = github_config.get('username', GITHUB_USERNAME)
        token = github_config.get('token', '')
        repo_name = github_config.get('defaultRepo', GITHUB_REPO)
        repo_url = github_config.get('repoUrl', f'https://github.com/{username}/{repo_name}.git')
    except Exception as e:
        print(f"âš ï¸ è¯»å– GitHub é…ç½®å¤±è´¥ï¼š{e}")
        return False
    
    today = datetime.now().strftime('%Y-%m-%d')
    tag_name = f'{GITHUB_TAG_PREFIX}{today}'
    repo_local_path = Path(f'/home/jjiao/.nanobot/workspace/{repo_name}')
    
    try:
        if not repo_local_path.exists():
            print(f"ğŸ“¥ å…‹éš†ä»“åº“ï¼š{repo_url}")
            auth_url = repo_url.replace('https://', f'https://{username}:{token}@')
            subprocess.run(['git', 'clone', auth_url, str(repo_local_path)], check=True, timeout=60)
        else:
            print(f"ğŸ”„ æ‹‰å–æœ€æ–°ä»£ç ï¼š{repo_local_path}")
            subprocess.run(['git', '-C', str(repo_local_path), 'pull'], check=True, timeout=30)
        
        report_filename = report_path.name
        dest_path = repo_local_path / report_filename
        shutil.copy(report_path, dest_path)
        print(f"ğŸ“„ å¤åˆ¶æŠ¥å‘Šï¼š{report_filename}")
        
        update_readme(str(repo_local_path), papers_count)
        
        gitignore_src = GITIGNORE_PATH
        gitignore_dest = repo_local_path / '.gitignore'
        if gitignore_src.exists() and not gitignore_dest.exists():
            shutil.copy(gitignore_src, gitignore_dest)
            print(f"ğŸ“„ å¤åˆ¶ .gitignore åˆ°ä»“åº“")
        
        subprocess.run(['git', '-C', str(repo_local_path), 'config', 'user.name', 'nanobot'], check=True)
        subprocess.run(['git', '-C', str(repo_local_path), 'config', 'user.email', 'nanobot@local'], check=True)
        subprocess.run(['git', '-C', str(repo_local_path), 'add', report_filename, 'README.md', '.gitignore'], check=True)
        subprocess.run(['git', '-C', str(repo_local_path), 'commit', '-m', f'ğŸ“š arXiv daily report {today}'], check=True)
        
        auth_url = repo_url.replace('https://', f'https://{username}:{token}@')
        subprocess.run(['git', '-C', str(repo_local_path), 'push', auth_url, GITHUB_BRANCH], check=True, timeout=60)
        print(f"âœ… æ¨é€åˆ° GitHub: {GITHUB_BRANCH} åˆ†æ”¯")
        
        result = subprocess.run(['git', '-C', str(repo_local_path), 'tag', '-l', tag_name], 
                               capture_output=True, text=True)
        if tag_name in result.stdout:
            print(f"âš ï¸ Tag {tag_name} å·²å­˜åœ¨ï¼Œåˆ é™¤åé‡æ–°åˆ›å»º")
            subprocess.run(['git', '-C', str(repo_local_path), 'tag', '-d', tag_name], check=True)
            subprocess.run(['git', '-C', str(repo_local_path), 'push', auth_url, '--delete', tag_name], 
                          capture_output=True)
        
        subprocess.run(['git', '-C', str(repo_local_path), 'tag', tag_name, '-m', f'arXiv daily report {today}'], check=True)
        subprocess.run(['git', '-C', str(repo_local_path), 'push', auth_url, tag_name], check=True, timeout=30)
        print(f"ğŸ·ï¸ åˆ›å»ºå¹¶æ¨é€ Tag: {tag_name}")
        
        return True
        
    except subprocess.CalledProcessError as e:
        print(f"âŒ Git æ“ä½œå¤±è´¥ï¼š{e}")
        return False
    except subprocess.TimeoutExpired as e:
        print(f"âŒ Git æ“ä½œè¶…æ—¶ï¼š{e}")
        return False
    except Exception as e:
        print(f"âŒ GitHub æ¨é€å¤±è´¥ï¼š{e}")
        return False


def main():
    """ä¸»å‡½æ•°"""
    output_dir = ARXIV_MONITOR_DIR
    json_path = output_dir / 'papers_history.json'
    
    today = datetime.now().strftime('%Y-%m-%d')
    report_path = output_dir / f'arxiv_daily_report_{today}.md'
    
    print(f"ğŸ” å¼€å§‹æœç´¢ arXiv Robotics è®ºæ–‡...")
    
    # ä»é…ç½®åŠ è½½æœç´¢æŸ¥è¯¢
    SEARCH_QUERIES = CONFIG.get('search_queries', [])
    
    all_papers = []
    for query in SEARCH_QUERIES:
        print(f"  æœç´¢ï¼š{query[:60]}...")
        papers = search_arxiv(query, max_results=50)
        filtered = filter_papers(papers, KEYWORDS)
        all_papers.extend(filtered)
        print(f"    æ‰¾åˆ° {len(filtered)} ç¯‡ç›¸å…³è®ºæ–‡")
    
    seen_ids = set()
    unique_papers = []
    today_dt = datetime.now()
    
    for p in all_papers:
        if p['arxiv_id'] in seen_ids:
            continue
        
        try:
            published_date = datetime.strptime(p['published'][:10], '%Y-%m-%d')
            days_diff = (today_dt - published_date).days
            if days_diff > MAX_DAYS_OLD:
                continue
        except:
            pass
        
        seen_ids.add(p['arxiv_id'])
        unique_papers.append(p)
    
    highlight_papers = [p for p in unique_papers if is_highlight_paper(p)]
    normal_papers = [p for p in unique_papers if not is_highlight_paper(p)]
    
    final_papers = highlight_papers.copy()
    
    if len(final_papers) >= MAX_PAPERS:
        final_papers = final_papers[:MAX_PAPERS]
    else:
        remaining_slots = MAX_PAPERS - len(final_papers)
        final_papers.extend(normal_papers[:remaining_slots])
    
    if len(final_papers) < MIN_PAPERS and len(normal_papers) > remaining_slots:
        additional_needed = MIN_PAPERS - len(final_papers)
        final_papers.extend(normal_papers[remaining_slots:remaining_slots + additional_needed])
    
    unique_papers = final_papers
    
    print(f"\nğŸ“Š å»é‡ + è¿‡æ»¤åå…± {len(unique_papers)} ç¯‡è®ºæ–‡ï¼ˆæœ€è¿‘ {MAX_DAYS_OLD} å¤©ï¼ŒèŒƒå›´ {MIN_PAPERS}-{MAX_PAPERS} ç¯‡ï¼‰")
    print(f"   - Highlight: {len(highlight_papers)} ç¯‡ | Poster: {len(unique_papers) - len(highlight_papers)} ç¯‡")
    
    print(f"\nğŸ” è·å–å¼€æºçŠ¶æ€...")
    for i, paper in enumerate(unique_papers):
        print(f"  [{i+1}/{len(unique_papers)}] {paper['title'][:50]}...")
        paper['open_source'] = detect_open_source(paper)
    
    print(f"\nğŸŒ å¼€å§‹ç¿»è¯‘æ‘˜è¦ä¸ºä¸­æ–‡ï¼ˆä½¿ç”¨ DeepSeekï¼‰...")
    for i, paper in enumerate(unique_papers):
        print(f"  [{i+1}/{len(unique_papers)}] ç¿»è¯‘ï¼š{paper['title'][:50]}...")
        chinese_summary = translate_to_chinese(paper['summary'])
        paper['chinese_summary'] = chinese_summary
    
    print(f"\nğŸ“¥ å¼€å§‹ä¸‹è½½é«˜äº®è®ºæ–‡ PDF...")
    downloaded_pdfs = []
    for i, paper in enumerate(unique_papers):
        if is_highlight_paper(paper):
            print(f"  [{i+1}/{len(unique_papers)}] é«˜äº®è®ºæ–‡ï¼š{paper['title'][:50]}...")
            pdf_path = download_pdf(paper)
            if pdf_path:
                downloaded_pdfs.append(pdf_path)
                paper['pdf_path'] = pdf_path
        else:
            print(f"  [{i+1}/{len(unique_papers)}] è·³è¿‡ï¼ˆéé«˜äº®ï¼‰ï¼š{paper['title'][:50]}...")
    
    print(f"\nğŸ’¾ å…±ä¸‹è½½ {len(downloaded_pdfs)} ç¯‡é«˜äº®è®ºæ–‡ PDF")
    if downloaded_pdfs:
        print(f"ğŸ“‚ PDF å­˜å‚¨ç›®å½•ï¼š{PDF_STORAGE_DIR}")
    
    new_count = save_results(unique_papers, json_path)
    print(f"\nğŸ’¾ ä¿å­˜æˆåŠŸï¼æ–°å¢ {new_count} ç¯‡ï¼Œå†å²å…± {len(unique_papers)} ç¯‡")
    
    generate_markdown_report(unique_papers, report_path, SEARCH_QUERIES)
    print(f"ğŸ“„ æŠ¥å‘Šå·²ç”Ÿæˆï¼š{report_path}")
    
    print(f"\nğŸ”‘ æœ¬æ¬¡æœç´¢å…³é”®è¯:")
    for i, kw in enumerate(SEARCH_QUERIES, 1):
        print(f"  {i}. {kw}")
    
    print(f"\nğŸš€ å¼€å§‹æ¨é€åˆ° GitHub...")
    github_success = push_to_github(report_path, len(unique_papers))
    if github_success:
        print(f"âœ… GitHub æ¨é€æˆåŠŸï¼")
    else:
        print(f"âŒ GitHub æ¨é€å¤±è´¥ï¼Œè¯·æ£€æŸ¥é…ç½®å’Œç½‘ç»œ")
    
    return unique_papers

if __name__ == '__main__':
    main()
