# ğŸ“š arXiv Robotics è®ºæ–‡æ—¥æŠ¥

**ç”Ÿæˆæ—¶é—´**: 2026-03-02

**ä»Šæ—¥æ–°å¢**: 20 ç¯‡

---

**ğŸ”“ å¼€æºä»£ç /æ¨¡å‹**: 0/20 ç¯‡æä¾›

**ğŸŒŸ Highlight**: 20 ç¯‡ | **ğŸ“Œ Poster**: 0 ç¯‡

---

## ğŸŒŸ Highlight Papers

### 1. UniScale: Unified Scale-Aware 3D Reconstruction for Multi-View Understanding via Prior Injection for Robotic Perception
- **Authors**: Mohammad Mahdavian, Gordon Tan, Binbin Xu et al.
- **Affiliation**: é™„å±å•ä½æœªæä¾›
- **Published**: 2026-02-26
- **arXiv**: [http://arxiv.org/abs/2602.23224v1](http://arxiv.org/abs/2602.23224v1)
- **Summary**: We present UniScale, a unified, scale-aware multi-view 3D reconstruction framework for robotic applications that flexibly integrates geometric priors through a modular, semantically informed design. In vision-based robotic navigation, the accurate extraction of environmental structure from raw image...

### 2. Latent Gaussian Splatting for 4D Panoptic Occupancy Tracking
- **Authors**: Maximilian Luz, Rohit Mohan, Thomas NÃ¼rnberg et al.
- **Affiliation**: é™„å±å•ä½æœªæä¾›
- **Published**: 2026-02-26
- **arXiv**: [http://arxiv.org/abs/2602.23172v1](http://arxiv.org/abs/2602.23172v1)
- **Summary**: Capturing 4D spatiotemporal surroundings is crucial for the safe and reliable operation of robots in dynamic environments. However, most existing methods address only one side of the problem: they either provide coarse geometric tracking via bounding boxes, or detailed 3D structures like voxel-based...

### 3. DySL-VLA: Efficient Vision-Language-Action Model Inference via Dynamic-Static Layer-Skipping for Robot Manipulation
- **Authors**: Zebin Yang, Yijiahao Qi, Tong Xie et al.
- **Affiliation**: é™„å±å•ä½æœªæä¾›
- **Published**: 2026-02-26
- **arXiv**: [http://arxiv.org/abs/2602.22896v1](http://arxiv.org/abs/2602.22896v1)
- **Summary**: Vision-Language-Action (VLA) models have shown remarkable success in robotic tasks like manipulation by fusing a language model's reasoning with a vision model's 3D understanding. However, their high computational cost remains a major obstacle for real-world applications that require real-time perfo...

### 4. GraspLDP: Towards Generalizable Grasping Policy via Latent Diffusion
- **Authors**: Enda Xiang, Haoxiang Ma, Xinzhu Ma et al.
- **Affiliation**: é™„å±å•ä½æœªæä¾›
- **Published**: 2026-02-26
- **arXiv**: [http://arxiv.org/abs/2602.22862v1](http://arxiv.org/abs/2602.22862v1)
- **Summary**: This paper focuses on enhancing the grasping precision and generalization of manipulation policies learned via imitation learning. Diffusion-based policy learning methods have recently become the mainstream approach for robotic manipulation tasks. As grasping is a critical subtask in manipulation, t...

### 5. ArtPro: Self-Supervised Articulated Object Reconstruction with Adaptive Integration of Mobility Proposals
- **Authors**: Xuelu Li, Zhaonan Wang, Xiaogang Wang et al.
- **Affiliation**: é™„å±å•ä½æœªæä¾›
- **Published**: 2026-02-26
- **arXiv**: [http://arxiv.org/abs/2602.22666v1](http://arxiv.org/abs/2602.22666v1)
- **Summary**: Reconstructing articulated objects into high-fidelity digital twins is crucial for applications such as robotic manipulation and interactive simulation. Recent self-supervised methods using differentiable rendering frameworks like 3D Gaussian Splatting remain highly sensitive to the initial part seg...

### 6. Rethinking the Practicality of Vision-language-action Model: A Comprehensive Benchmark and An Improved Baseline
- **Authors**: Wenxuan Song, Jiayi Chen, Xiaoquan Sun et al.
- **Affiliation**: é™„å±å•ä½æœªæä¾›
- **Published**: 2026-02-26
- **arXiv**: [http://arxiv.org/abs/2602.22663v1](http://arxiv.org/abs/2602.22663v1)
- **Summary**: Vision-Language-Action (VLA) models have emerged as a generalist robotic agent. However, existing VLAs are hindered by excessive parameter scales, prohibitive pre-training requirements, and limited applicability to diverse embodiments. To improve the practicality of VLAs, we propose a comprehensive ...

### 7. Metamorphic Testing of Vision-Language Action-Enabled Robots
- **Authors**: Pablo Valle, Sergio Segura, Shaukat Ali et al.
- **Affiliation**: é™„å±å•ä½æœªæä¾›
- **Published**: 2026-02-26
- **arXiv**: [http://arxiv.org/abs/2602.22579v1](http://arxiv.org/abs/2602.22579v1)
- **Summary**: Vision-Language-Action (VLA) models are multimodal robotic task controllers that, given an instruction and visual inputs, produce a sequence of low-level control actions (or motor commands) enabling a robot to execute the requested task in the physical environment. These systems face the test oracle...

### 8. SignVLA: A Gloss-Free Vision-Language-Action Framework for Real-Time Sign Language-Guided Robotic Manipulation
- **Authors**: Xinyu Tan, Ningwei Bai, Harry Gardener et al.
- **Affiliation**: é™„å±å•ä½æœªæä¾›
- **Published**: 2026-02-26
- **arXiv**: [http://arxiv.org/abs/2602.22514v1](http://arxiv.org/abs/2602.22514v1)
- **Summary**: We present, to our knowledge, the first sign language-driven Vision-Language-Action (VLA) framework for intuitive and inclusive human-robot interaction. Unlike conventional approaches that rely on gloss annotations as intermediate supervision, the proposed system adopts a gloss-free paradigm and dir...

### 9. When to Act, Ask, or Learn: Uncertainty-Aware Policy Steering
- **Authors**: Jessie Yuan, Yilin Wu, Andrea Bajcsy
- **Affiliation**: é™„å±å•ä½æœªæä¾›
- **Published**: 2026-02-25
- **arXiv**: [http://arxiv.org/abs/2602.22474v1](http://arxiv.org/abs/2602.22474v1)
- **Summary**: Policy steering is an emerging way to adapt robot behaviors at deployment-time: a learned verifier analyzes low-level action samples proposed by a pre-trained policy (e.g., diffusion policy) and selects only those aligned with the task. While Vision-Language Models (VLMs) are promising general-purpo...

### 10. Are Foundation Models the Route to Full-Stack Transfer in Robotics?
- **Authors**: Freek Stulp, Samuel Bustamante, JoÃ£o SilvÃ©rio et al.
- **Affiliation**: é™„å±å•ä½æœªæä¾›
- **Published**: 2026-02-25
- **arXiv**: [http://arxiv.org/abs/2602.22001v1](http://arxiv.org/abs/2602.22001v1)
- **Summary**: In humans and robots alike, transfer learning occurs at different levels of abstraction, from high-level linguistic transfer to low-level transfer of motor skills. In this article, we provide an overview of the impact that foundation models and transformer networks have had on these different levels...

---

## ğŸ“Œ Poster Papers


---

*æœ€åæ›´æ–°ï¼š2026-03-02*
